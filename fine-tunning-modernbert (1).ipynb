{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":" # Ensure ModernBERT support","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T15:10:16.453920Z","iopub.execute_input":"2025-12-25T15:10:16.454301Z","iopub.status.idle":"2025-12-25T15:10:53.279545Z","shell.execute_reply.started":"2025-12-25T15:10:16.454275Z","shell.execute_reply":"2025-12-25T15:10:53.278822Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m521.0/521.0 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25h  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nsentence-transformers 5.1.1 requires transformers<5.0.0,>=4.41.0, but you have transformers 5.0.0.dev0 which is incompatible.\ngradio 5.49.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.5 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"pip install \"transformers>=4.41.0,<5.0.0\" \"pydantic>=2.0,<2.12\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T15:15:52.015112Z","iopub.execute_input":"2025-12-25T15:15:52.015483Z","iopub.status.idle":"2025-12-25T15:16:07.177477Z","shell.execute_reply.started":"2025-12-25T15:15:52.015451Z","shell.execute_reply":"2025-12-25T15:16:07.176749Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Collecting transformers<5.0.0,>=4.41.0\n  Downloading transformers-4.57.3-py3-none-any.whl.metadata (43 kB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting pydantic<2.12,>=2.0\n  Downloading pydantic-2.11.10-py3-none-any.whl.metadata (68 kB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m68.6/68.6 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0) (3.20.1)\nCollecting huggingface-hub<1.0,>=0.34.0 (from transformers<5.0.0,>=4.41.0)\n  Downloading huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0) (2.0.2)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0) (6.0.3)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0) (2025.11.3)\nRequirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0) (2.32.5)\nRequirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0) (0.22.1)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0) (0.6.2)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0) (4.67.1)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<2.12,>=2.0) (0.7.0)\nCollecting pydantic-core==2.33.2 (from pydantic<2.12,>=2.0)\n  Downloading pydantic_core-2.33.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\nRequirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<2.12,>=2.0) (4.15.0)\nRequirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<2.12,>=2.0) (0.4.2)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers<5.0.0,>=4.41.0) (2025.10.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers<5.0.0,>=4.41.0) (1.2.1rc0)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers<5.0.0,>=4.41.0) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers<5.0.0,>=4.41.0) (3.11)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers<5.0.0,>=4.41.0) (2.6.2)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers<5.0.0,>=4.41.0) (2025.11.12)\nDownloading transformers-4.57.3-py3-none-any.whl (12.0 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m125.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:01\u001b[0m\n\u001b[?25hDownloading pydantic-2.11.10-py3-none-any.whl (444 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m444.8/444.8 kB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pydantic_core-2.33.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m75.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m566.1/566.1 kB\u001b[0m \u001b[31m38.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: pydantic-core, pydantic, huggingface-hub, transformers\n  Attempting uninstall: pydantic-core\n    Found existing installation: pydantic_core 2.41.5\n    Uninstalling pydantic_core-2.41.5:\n      Successfully uninstalled pydantic_core-2.41.5\n  Attempting uninstall: pydantic\n    Found existing installation: pydantic 2.12.5\n    Uninstalling pydantic-2.12.5:\n      Successfully uninstalled pydantic-2.12.5\n  Attempting uninstall: huggingface-hub\n    Found existing installation: huggingface_hub 1.2.3\n    Uninstalling huggingface_hub-1.2.3:\n      Successfully uninstalled huggingface_hub-1.2.3\n  Attempting uninstall: transformers\n    Found existing installation: transformers 5.0.0.dev0\n    Uninstalling transformers-5.0.0.dev0:\n      Successfully uninstalled transformers-5.0.0.dev0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nlangchain 0.3.27 requires SQLAlchemy<3,>=1.4, but you have sqlalchemy 1.2.19 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed huggingface-hub-0.36.0 pydantic-2.11.10 pydantic-core-2.33.2 transformers-4.57.3\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"# GoEmotions Dataset Preprocessing for Multi-Label Classification\n\n## ğŸ“Œ Overview\nThis notebook prepares the **GoEmotions dataset** for multi-label emotion classification tasks using deep learning models. The dataset consists of Reddit comments annotated with multiple fine-grained emotions.\n\n---\n\n## ğŸ“Š Dataset Details\n- **Source:** Google Research\n- **Dataset:** GoEmotions (Simplified)\n- **Total Labels:** 28 (27 emotions + neutral)\n- **Task Type:** Multi-label classification\n- **Input:** Text (Reddit comments)\n- **Output:** Binary emotion vectors\n\n---\n\n## âš™ï¸ Preprocessing Steps\n\n### 1. Load Dataset\nThe dataset is loaded using the Hugging Face `datasets` library with the `simplified` configuration to remove rare labels.\n\n### 2. Extract Emotion Labels\nAll emotion names are extracted from the dataset schema to maintain a consistent label order.\n\n### 3. Create Label Mappings\n- `label2id`: Maps emotion names to numeric IDs\n- `id2label`: Maps numeric IDs back to emotion names\n\nThese mappings are essential for model training and evaluation.\n\n### 4. Multi-Label One-Hot Encoding\nEach text sample can express **multiple emotions**.  \nTo support this, labels are converted into **binary vectors**:\n\n- Vector length = number of emotions\n- `1.0` indicates presence of an emotion\n- `0.0` indicates absence\n\n#### Example:\nEmotion indices: `[2, 5]`\n\nBinary vector:\n","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\nimport numpy as np\n\n# Load the dataset\ndataset = load_dataset(\"google-research-datasets/go_emotions\", \"simplified\")\n\n# Get labels\nlabels = dataset[\"train\"].features[\"labels\"].feature.names\nid2label = {i: label for i, label in enumerate(labels)}\nlabel2id = {label: i for i, label in enumerate(labels)}\n\ndef preprocess_data(examples):\n    # Convert labels to a binary vector (One-Hot Encoding for Multi-label)\n    batch_labels = []\n    for labels_list in examples[\"labels\"]:\n        binary_labels = [0.0] * len(labels)\n        for l in labels_list:\n            binary_labels[l] = 1.0\n        batch_labels.append(binary_labels)\n    \n    examples[\"label_vec\"] = batch_labels\n    return examples\n\ndataset = dataset.map(preprocess_data, batched=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T15:17:10.530967Z","iopub.execute_input":"2025-12-25T15:17:10.531324Z","iopub.status.idle":"2025-12-25T15:17:21.597421Z","shell.execute_reply.started":"2025-12-25T15:17:10.531293Z","shell.execute_reply":"2025-12-25T15:17:21.596626Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3be2a28387814db58bb432d7be78edfa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"simplified/train-00000-of-00001.parquet:   0%|          | 0.00/2.77M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"18b386425999430aaa8d44bce9a3a1ea"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"simplified/validation-00000-of-00001.par(â€¦):   0%|          | 0.00/350k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e5ddb0b526cb41d189d71f48e3c63d96"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"simplified/test-00000-of-00001.parquet:   0%|          | 0.00/347k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0e54c83e359b4eb0b1c91c865acf268b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/43410 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"caa5e6de2e1a4a288b3e307744510058"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/5426 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"306f94dfc9d047b29b98d05646016287"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/5427 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1ebf5b9a0e874aa391d7a7ed25a02804"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/43410 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7e5a95d8ae6049428d51ec61d4a2bc22"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/5426 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"984d7a103a2f48d484ef1a0157b50364"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/5427 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9397ec32ab624958885150a5d822fe3c"}},"metadata":{}}],"execution_count":3},{"cell_type":"markdown","source":"# Tokenization and Dataset Preparation for Transformer Training\n\n## ğŸ“Œ Overview\nThis section tokenizes the GoEmotions dataset using a BERT-compatible tokenizer and prepares it for efficient PyTorch-based training.\n\n---\n\n## ğŸ”¤ Tokenizer\n- **Model:** ModernBERT-base\n- **Tokenizer:** AutoTokenizer\n- Ensures vocabulary and token embeddings are aligned with the pretrained model.\n\n---\n\n## âš™ï¸ Tokenization Strategy\n\n### Configuration\n- **Max sequence length:** 128\n- **Padding:** Fixed-length (`max_length`)\n- **Truncation:** Enabled for long texts\n\nThis balances semantic coverage with computational efficiency.\n\n---\n\n## ğŸ”„ Dataset Transformation\n\n### Steps Applied\n1. Convert raw text into:\n   - `input_ids`\n   - `attention_mask`\n2. Remove unused columns (`text`, `labels`, `id`)\n3. Rename multi-label vector column:\n   - `label_vec` â†’ `labels`\n4. Convert all features to `torch.Tensor`\n\n---\n\n## âœ… Final Dataset Format\nEach training sample contains:\n```python\n{\n  \"input_ids\": Tensor,\n  \"attention_mask\": Tensor,\n  \"labels\": Tensor  # multi-hot emotion vector\n}\n","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\nmodel_id = \"answerdotai/ModernBERT-base\"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n\ndef tokenize_function(examples):\n    return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n\ntokenized_dataset = dataset.map(tokenize_function, batched=True)\ntokenized_dataset = tokenized_dataset.remove_columns([\"text\", \"labels\", \"id\"])\ntokenized_dataset = tokenized_dataset.rename_column(\"label_vec\", \"labels\")\ntokenized_dataset.set_format(\"torch\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T15:18:05.535353Z","iopub.execute_input":"2025-12-25T15:18:05.535884Z","iopub.status.idle":"2025-12-25T15:18:17.106885Z","shell.execute_reply.started":"2025-12-25T15:18:05.535852Z","shell.execute_reply":"2025-12-25T15:18:17.106321Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f677a72adb634e4c82db9fe0e35a0083"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"033d78b8bc6c41b99e48b350f26093e8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/694 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e8be7804d7a942ccb58b152eb8b50a67"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/43410 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2a4c3766f2564a8781504373a007f2df"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/5426 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e692e9b0d13d46c688d9ae7b2d83abe3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/5427 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"276af2ac359a46e197d4a397571b9636"}},"metadata":{}}],"execution_count":4},{"cell_type":"markdown","source":"# Fine-Tuning ModernBERT for Multi-Label Emotion Classification\n\n## ğŸ“Œ Overview\nThis section fine-tunes **ModernBERT-base** on the **GoEmotions dataset** for **multi-label emotion classification** using Hugging Faceâ€™s `Trainer` API.\n\nEach input text may express multiple emotions simultaneously, requiring a sigmoid-based output layer and binary cross-entropy loss.\n\n---\n\n## ğŸ§  Model Configuration\n- **Base Model:** ModernBERT-base\n- **Architecture:** Encoder-only Transformer\n- **Task:** Multi-label classification\n- **Number of Labels:** 28 (27 emotions + neutral)\n- **Activation:** Sigmoid\n- **Loss Function:** Binary Cross-Entropy with Logits\n\n---\n\n## ğŸ“Š Evaluation Metrics\nTo correctly evaluate multi-label predictions, the following metrics are used:\n\n- **Weighted F1 Score:** Handles label imbalance\n- **ROC-AUC (Weighted):** Measures ranking quality across all labels\n\nAccuracy is intentionally avoided due to its poor interpretability in multi-label settings.\n\n---\n\n## âš™ï¸ Training Configuration\n\n| Parameter | Value |\n|---------|------|\n| Learning Rate | 3e-5 |\n| Batch Size | 32 |\n| Epochs | 4 |\n| Weight Decay | 0.01 |\n| Precision | fp16 (mixed) |\n| Evaluation | Per epoch |\n| Best Model Selection | Weighted F1 |\n\nAdditional optimizations:\n- Grouping sequences by length to reduce padding\n- Mixed precision for faster training and lower memory usage\n\n---\n\n## ğŸš€ Training Pipeline\n1. Tokenize input text using ModernBERT tokenizer\n2. Convert emotion labels to multi-hot vectors\n3. Fine-tune all model layers end-to-end\n4. Evaluate after each epoch\n5. Automatically restore best-performing checkpoint\n\n---\n\n## âœ… Outcome\nThe resulting model learns to predict **multiple fine-grained emotions per text** and is suitable for deployment in:\n- Emotion-aware chat systems\n- Content moderation pipelines\n- Affective computing research\n\n---\n\n## ğŸ“¦ Deployment Ready\nThe trained model is fully compatible with:\n- Hugging Face Hub\n- Transformers inference pipelines\n- PyTorch deployment environments\n","metadata":{}},{"cell_type":"code","source":"from transformers import ModernBertForSequenceClassification, TrainingArguments, Trainer\nimport torch\nimport numpy as np\nfrom sklearn.metrics import f1_score, roc_auc_score\n\nmodel = ModernBertForSequenceClassification.from_pretrained(\n    model_id, \n    num_labels=len(labels),\n    problem_type=\"multi_label_classification\",\n    id2label=id2label,\n    label2id=label2id,\n    reference_compile=False,   \n    attn_implementation=\"eager\" \n)\n\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    # Use torch logic or numpy for sigmoid\n    probs = 1 / (1 + np.exp(-logits))\n    predictions = (probs > 0.5).astype(float)\n    \n    # Ensure labels are float for consistency with predictions\n    labels = labels.astype(float)\n    \n    f1_weighted = f1_score(labels, predictions, average=\"weighted\", zero_division=0)\n    roc_auc = roc_auc_score(labels, probs, average=\"weighted\")\n    \n    return {\"f1\": f1_weighted, \"roc_auc\": roc_auc}\n\ntraining_args = TrainingArguments(\n    output_dir=\"./ModernBERT-GoEmotions\",\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    learning_rate=3e-5, \n    per_device_train_batch_size=32,\n    per_device_eval_batch_size=32,\n    num_train_epochs=4,\n    weight_decay=0.01,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"f1\",\n    fp16=True, \n    report_to=\"none\",\n    # Added: helps with Kaggle memory management\n    dataloader_num_workers=2, \n    group_by_length=True \n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset[\"train\"],\n    eval_dataset=tokenized_dataset[\"validation\"],\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics,\n)\n\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T15:32:44.768514Z","iopub.execute_input":"2025-12-25T15:32:44.768880Z","iopub.status.idle":"2025-12-25T16:21:23.530823Z","shell.execute_reply.started":"2025-12-25T15:32:44.768840Z","shell.execute_reply":"2025-12-25T16:21:23.530085Z"}},"outputs":[{"name":"stderr","text":"Some weights of ModernBertForSequenceClassification were not initialized from the model checkpoint at answerdotai/ModernBERT-base and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/tmp/ipykernel_55/805928642.py:48: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\nThe tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': None, 'bos_token_id': None}.\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='5428' max='5428' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [5428/5428 48:28, Epoch 4/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>F1</th>\n      <th>Roc Auc</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.090100</td>\n      <td>0.083718</td>\n      <td>0.525910</td>\n      <td>0.911941</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.075400</td>\n      <td>0.080864</td>\n      <td>0.552808</td>\n      <td>0.915667</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.059100</td>\n      <td>0.085238</td>\n      <td>0.567170</td>\n      <td>0.909428</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.039100</td>\n      <td>0.093749</td>\n      <td>0.569230</td>\n      <td>0.902828</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=5428, training_loss=0.06804641310397368, metrics={'train_runtime': 2908.9135, 'train_samples_per_second': 59.692, 'train_steps_per_second': 1.866, 'total_flos': 1.479496440582144e+16, 'train_loss': 0.06804641310397368, 'epoch': 4.0})"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"model.save_pretrained(\"./modernbert-goemotions\")\ntokenizer.save_pretrained(\"./modernbert-goemotions\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T16:55:52.382265Z","iopub.execute_input":"2025-12-25T16:55:52.383130Z","iopub.status.idle":"2025-12-25T16:55:53.407152Z","shell.execute_reply.started":"2025-12-25T16:55:52.383090Z","shell.execute_reply":"2025-12-25T16:55:53.406360Z"}},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"('./modernbert-goemotions/tokenizer_config.json',\n './modernbert-goemotions/special_tokens_map.json',\n './modernbert-goemotions/tokenizer.json')"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"from huggingface_hub import notebook_login\nnotebook_login() # Paste your token here\n\n# Push to Hub\nmodel.push_to_hub(\"Pradeep-mahato/ModernBERT-GoEmotions\")\ntokenizer.push_to_hub(\"Pradeep-mahato/ModernBERT-GoEmotions\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T16:57:16.183570Z","iopub.execute_input":"2025-12-25T16:57:16.184328Z","iopub.status.idle":"2025-12-25T16:57:32.833577Z","shell.execute_reply.started":"2025-12-25T16:57:16.184295Z","shell.execute_reply":"2025-12-25T16:57:32.832950Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b0c72be11a334a5a904ecd7e6fd00854"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Processing Files (0 / 0): |          |  0.00B /  0.00B            ","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6ac8ee8f091c4938af8ba1fa3f9cf2aa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"New Data Upload: |          |  0.00B /  0.00B            ","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bc2973845e4d4566ac6626ba5a991abd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ed074f8bff2d4b5781049dd1403d326a"}},"metadata":{}},{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/Pradeep-mahato/ModernBERT-GoEmotions/commit/701b348eaba392814fb9e4d10193fc3ad3b25dec', commit_message='Upload tokenizer', commit_description='', oid='701b348eaba392814fb9e4d10193fc3ad3b25dec', pr_url=None, repo_url=RepoUrl('https://huggingface.co/Pradeep-mahato/ModernBERT-GoEmotions', endpoint='https://huggingface.co', repo_type='model', repo_id='Pradeep-mahato/ModernBERT-GoEmotions'), pr_revision=None, pr_num=None)"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"from transformers import pipeline\n\n# 1. Initialize the pipeline using your trained model and tokenizer\n\npipe = pipeline(\n    \"text-classification\", \n    model=model, \n    tokenizer=tokenizer, \n    top_k=None, # Returns all labels (we will filter them)\n    device=0    # Use GPU\n)\n\ndef predict_emotions(text, threshold=0.4):\n    \"\"\"\n    Predicts emotions for a given text and filters by confidence threshold.\n    \"\"\"\n    results = pipe(text)[0]\n    # Filter results based on the threshold\n    emotions = [res for res in results if res['score'] >= threshold]\n    \n    print(f\"\\nText: {text}\")\n    if not emotions:\n        print(\"Result: Neutral/No strong emotion detected.\")\n    else:\n        for e in emotions:\n            print(f\" - {e['label']}: {e['score']:.4f}\")\n\n# 2. Test it out!\ntest_sentences = [\n    \"I am so proud of how this model turned out, ModernBERT is amazing!\",\n    \"I'm feeling a bit nervous about the deployment tomorrow.\",\n    \"This is the worst bug I have ever encountered. Why won't it work?\"\n]\n\nfor sentence in test_sentences:\n    predict_emotions(sentence)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T17:16:31.628663Z","iopub.execute_input":"2025-12-25T17:16:31.629447Z","iopub.status.idle":"2025-12-25T17:16:31.915288Z","shell.execute_reply.started":"2025-12-25T17:16:31.629407Z","shell.execute_reply":"2025-12-25T17:16:31.914574Z"}},"outputs":[{"name":"stderr","text":"Device set to use cuda:0\n","output_type":"stream"},{"name":"stdout","text":"\nText: I am so proud of how this model turned out, ModernBERT is amazing!\n - admiration: 0.9882\n - pride: 0.4005\n\nText: I'm feeling a bit nervous about the deployment tomorrow.\n - nervousness: 0.9413\n\nText: This is the worst bug I have ever encountered. Why won't it work?\n - disgust: 0.4115\n","output_type":"stream"}],"execution_count":13}]}